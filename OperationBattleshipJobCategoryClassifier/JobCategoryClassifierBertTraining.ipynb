{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Job titles come in many different forms but can often be representing basic types of jobs. We need the ability to reduce the noise and associate various job titles with discrete job categories. For instance a Product Manager job could be written as:\n",
    "- Product Manager\n",
    "- Sr Product Manager\n",
    "- AI Product Manager\n",
    "- Product Manager - Platform\n",
    "- etc....\n",
    "\n",
    "We will fine tune a Distilbert Model so that it can predict the job title category based on the text written in the job description. We will save the fine tuned model and then call if from other Python Modules in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Information about DistilBert\n",
    "\n",
    "\n",
    "### References:\n",
    "- https://www.youtube.com/watch?v=ZvsH09XGuZ0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "\n",
    "### Step One: Ensure all necessary libraries have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.5.0\n",
    "!pip install accelerate transformers[torch]\n",
    "!pip install datasets boto3 python-dotenv joblib requests\n",
    "!pip install psycopg2-binary\n",
    "!pip install pandas apify-client nomic openai pinecone-client\n",
    "!pip install --no-deps OperationBattleshipCommonUtilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from joblib import dump, load\n",
    "\n",
    "from operation_battleship_common_utilities.JobPostingDao import JobPostingDao\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the environment variables. \n",
    "\n",
    "- We can load the .env file if running locally.\n",
    "- When running on Paperspace, I hit barriers with importing the .env file so I elected to manually set them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DigitalOcean configuration\n",
    "os.environ['digitalOcean'] = 'update_value'\n",
    "os.environ['password'] = 'update_value'\n",
    "os.environ['host'] = 'update_value'\n",
    "os.environ['port'] = 'update_value'\n",
    "os.environ['database'] = 'update_value'\n",
    "os.environ['sslmode'] = 'update_value'\n",
    "\n",
    "# Logging configuration\n",
    "os.environ['LOG_LEVEL'] = 'update_value'\n",
    "os.environ['LOG_FILE'] = 'update_value'\n",
    "\n",
    "# DigitalOcean Spaces credentials\n",
    "os.environ['DO_ACCESS_KEY'] = 'update_value'\n",
    "os.environ['DO_SECRET_KEY'] = 'update_value'\n",
    "os.environ['DO_REGION'] = 'update_value'\n",
    "os.environ['DO_BUCKET_NAME'] = 'update_value'\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'update_value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopw = stopwords.words('english')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_jobs_with_required_columns():  ##\n",
    "    print(\"Fetching jobs with required columns\")\n",
    "\n",
    "    job_posting_dao = JobPostingDao()\n",
    "    all_jobs_df = job_posting_dao.getAllJobs()\n",
    "\n",
    "    columns_to_keep = [\n",
    "        'job_posting_id',\n",
    "        'job_title',\n",
    "        'full_posting_description',\n",
    "        'job_category'\n",
    "    ]\n",
    "\n",
    "    all_jobs_df = all_jobs_df[columns_to_keep]\n",
    "\n",
    "    ordered_columns = [\n",
    "        'job_posting_id',\n",
    "        'job_category',\n",
    "        'job_title',\n",
    "        'full_posting_description'\n",
    "    ]\n",
    "\n",
    "    all_jobs_df = all_jobs_df[ordered_columns]\n",
    "\n",
    "    all_jobs_df = all_jobs_df.sample(frac=1).reset_index(drop=True) # Randomly shuffle the records in the DF\n",
    "\n",
    "    return all_jobs_df\n",
    "\n",
    "def remove_empty_and_short_rows(df): ##\n",
    "    print(\"Removing data for inference\")\n",
    "    df = df.dropna(subset=['job_category'])\n",
    "    df = df[df['job_category'] != 'Unknown']\n",
    "\n",
    "    df = df.dropna(subset=['full_posting_description'])\n",
    "    df = df[df['full_posting_description'].str.strip() != '']\n",
    "\n",
    "    df = df.dropna(subset=['full_posting_description'])\n",
    "    df['word_count'] = df['full_posting_description'].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df['word_count'] >= 80]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def encodeJobCategories(df, le): ##\n",
    "    df['encoded_categories'] = le.transform(df['job_category'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text): ##\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    return text\n",
    "\n",
    "def upload_to_s3(file_path, bucket_name, object_name):\n",
    "    s3_client = boto3.client('s3',\n",
    "                             region_name= os.environ['DO_REGION'],\n",
    "                             aws_access_key_id= os.environ['DO_ACCESS_KEY'],\n",
    "                             aws_secret_access_key= os.environ['DO_SECRET_KEY'])\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, os.environ['DO_BUCKET_NAME'], object_name)\n",
    "        print(f\"File {file_path} uploaded to {bucket_name}/{object_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "\n",
    "def download_from_s3(url, destination):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        with open(destination, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File downloaded from {url} to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download file from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define the functions for measuring and benchmarking the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "def evaluate_model(trainer, test_dataset, label_encoder):\n",
    "    # Get predictions\n",
    "    preds_output = trainer.predict(test_dataset)\n",
    "    predictions = np.argmax(preds_output.predictions, axis=1)\n",
    "    true_labels = preds_output.label_ids\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Training and Validation Loss Curves\n",
    "    training_logs = trainer.state.log_history\n",
    "\n",
    "    train_loss = [x['loss'] for x in training_logs if 'loss' in x.keys()]\n",
    "    eval_loss = [x['eval_loss'] for x in training_logs if 'eval_loss' in x.keys()]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(eval_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the model. \n",
    "\n",
    "1. Get the list of jobs with labels\n",
    "2. Encode the job_category\n",
    "3. Remove stop words and unnecessary characters from the Job Title\n",
    "4. Define the model\n",
    "5. Train the model\n",
    "6. Benchmark the model\n",
    "7. Persist the model to Digital Ocean S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        start_time = time.time()  # Start timer\n",
    "\n",
    "        # Get the jobs from the DB and drop all unnecessary columns\n",
    "        jobsDf = get_jobs_with_required_columns()\n",
    "        if jobsDf is None or jobsDf.empty:\n",
    "            print(\"No job data retrieved or DataFrame is empty\")\n",
    "            return\n",
    "\n",
    "        jobsDf = remove_empty_and_short_rows(jobsDf)\n",
    "        if jobsDf is None or jobsDf.empty:\n",
    "            print(\"No job data\")\n",
    "            return\n",
    "\n",
    "        # Download and load the label encoder\n",
    "        label_encoder_path = 'label_encoder.joblib'\n",
    "        download_from_s3(\"https://operationbattleship-resumes.nyc3.cdn.digitaloceanspaces.com/label_encoder.joblib\", label_encoder_path)\n",
    "        label_encoder = load(label_encoder_path)\n",
    "        print(\"Label encoder loaded.\")\n",
    "\n",
    "        jobsDf = encodeJobCategories(jobsDf, label_encoder)\n",
    "        if jobsDf is None or jobsDf.empty:\n",
    "            print(\"No job data after category encoding\")\n",
    "            return\n",
    "\n",
    "        print(f\"Data Prep Complete\")\n",
    "        print(f\"Number of rows in jobsDf: {jobsDf.shape[0]}\")\n",
    "\n",
    "        jobsDf = jobsDf.reset_index(drop=True)\n",
    "\n",
    "        jobsDf['job_title'] = jobsDf['job_title'].apply(clean_text)\n",
    "\n",
    "        data_texts = jobsDf[\"job_title\"].tolist()\n",
    "        data_labels = jobsDf[\"encoded_categories\"].tolist()\n",
    "\n",
    "        print(f\"Type of object for data_texts: {type(data_texts)}\")\n",
    "        print(f\"Type of object for data_labels: {type(data_labels)}\")\n",
    "\n",
    "        # Tokenize data using DistilBERT tokenizer\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        encodings = tokenizer(data_texts, truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # Create a Dataset from the tokenized inputs and labels\n",
    "        dataset = Dataset.from_dict({\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': data_labels\n",
    "        })\n",
    "\n",
    "        train_testvalid = dataset.train_test_split(test_size=0.2)\n",
    "        train_testvalid['validation'] = train_testvalid.pop('test').train_test_split(test_size=0.5)\n",
    "\n",
    "        train_dataset = train_testvalid['train']\n",
    "        val_dataset = train_testvalid['validation']['test']\n",
    "        test_dataset = train_testvalid['validation']['train']\n",
    "\n",
    "        # Define model\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(np.unique(data_labels)))\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=5,  # Increased number of epochs for potentially better performance\n",
    "            per_device_train_batch_size=8,  # Adjust based on your GPU memory\n",
    "            per_device_eval_batch_size=8,  # Adjust based on your GPU memory\n",
    "            warmup_steps=100,  # Adjusted for a shorter warm-up period\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=50,  # Adjust logging steps to reduce overhead\n",
    "            evaluation_strategy=\"steps\",  # Change to \"steps\" if you want more frequent evaluations\n",
    "            eval_steps=500,  # Evaluate every 500 steps\n",
    "            save_strategy=\"steps\",  # Save model more frequently\n",
    "            save_steps=500,  # Save every 500 steps\n",
    "            save_total_limit=2,  # Keep only the last 2 models to save disk space\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"none\",\n",
    "            fp16=True,  # Enable mixed precision training\n",
    "            gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n",
    "            learning_rate=5e-5,  # Explicitly set learning rate\n",
    "            logging_first_step=True,  # Log the first step to monitor initial progress\n",
    "            metric_for_best_model=\"accuracy\",  # Assuming accuracy is your key metric\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics  # Pass the compute_metrics function\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model locally\n",
    "        model_path = 'distilbert_model'\n",
    "        trainer.save_model(model_path)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "\n",
    "        # Upload the model to S3\n",
    "        for root, _, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                upload_to_s3(os.path.join(root, file), os.environ['DO_BUCKET_NAME'], os.path.relpath(os.path.join(root, file), model_path))\n",
    "\n",
    "        print(f\"Model uploaded to S3 successfully.\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        with open('results/distilbert_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "        upload_to_s3('results/distilbert_metrics.json', os.environ['DO_BUCKET_NAME'], 'results/distilbert_metrics.json')\n",
    "        \n",
    "        evaluate_model(trainer, test_dataset, label_encoder)\n",
    "\n",
    "        end_time = time.time()  # End timer\n",
    "        duration = (end_time - start_time) / 60  # Duration in minutes\n",
    "\n",
    "        print(f\"Script complete. Time taken to complete: {duration} minutes\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
